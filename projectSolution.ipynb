{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections import defaultdict\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aSize = env.action_space.n\n",
    "sSize = env.observation_space.n\n",
    "\n",
    "qTable = np.zeros((sSize, aSize))\n",
    "\n",
    "episodesCount = 10000\n",
    "maxStep = 100\n",
    "\n",
    "learningRate = 0.1  \n",
    "discountRate = 0.99  \n",
    "\n",
    "explorationRate = 1\n",
    "maxExploration = 1\n",
    "minExploration = 0.01\n",
    "eDecay = 0.001\n",
    "\n",
    "qTables = []\n",
    "intervals = [0, 1000, 5000, 10000]  \n",
    "\n",
    "rewardsList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if episodesCount not in intervals:\n",
    "    intervals.append(episodesCount)\n",
    "\n",
    "rewardsList = []\n",
    "errorList = []\n",
    "\n",
    "for episode in range(episodesCount):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    currentReward = 0\n",
    "    totalError = 0  \n",
    "    \n",
    "    for step in range(maxStep): \n",
    "        \n",
    "        explorationRate_threshold = random.uniform(0, 1)\n",
    "        if explorationRate_threshold > explorationRate:\n",
    "            action = np.argmax(qTable[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        oldQ = qTable[state, action]\n",
    "        currently, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        winnersMove = np.max(qTable[currently, :])\n",
    "        newQ = (1 - learningRate) * oldQ + learningRate * (reward + discountRate * winnersMove)\n",
    "        qTable[state, action] = newQ\n",
    "        \n",
    "        error = abs(newQ - oldQ)\n",
    "        totalError += error\n",
    "        \n",
    "        state = currently\n",
    "        currentReward += reward\n",
    "        \n",
    "        if done: \n",
    "            break\n",
    "            \n",
    "    explorationRate = minExploration + \\\n",
    "        (maxExploration - minExploration) * np.exp(-eDecay * episode)\n",
    "    \n",
    "    rewardsList.append(currentReward)\n",
    "    errorList.append(totalError)  \n",
    "    \n",
    "    if episode in intervals:\n",
    "        qTables.append(qTable.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRE(rewardsList, errorList):\n",
    "    episodes = range(len(rewardsList))\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    ax1.set_xlabel('Episodes')\n",
    "    ax1.set_ylabel('Reward')\n",
    "    ax1.plot(episodes, rewardsList, color='tab:green', label='Reward per Episode')\n",
    "    ax1.tick_params(axis='y')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Training Error', color='tab:blue')\n",
    "    ax2.plot(episodes, errorList, color='tab:blue', label='Error per Episode')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    fig.suptitle('Rewards and Errors Over Time', fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plotRE(rewardsList, errorList)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
